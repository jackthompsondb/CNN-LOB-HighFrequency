{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "from CNNimagegen import create_image_updown_vol\n",
    "from CNNimagegen import create_image_side_vol\n",
    "from CNNimagegen import create_image_updown_price\n",
    "from CNNimagegen import create_image_updown_vol_mid\n",
    "from CNNimagegen import create_image_updown_vol_mid_triple_l10\n",
    "from CNNimagegen import create_image_updown_vol_mid_triple_l5\n",
    "from CNNimagegen import create_image_updown_vol_mid_triple_l3\n",
    "from CNNimagegen import data_frame_organise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dbjac\\Desktop\\CNN\\CNNimagegen.py:14: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method='ffill')  # Forward fill\n",
      "c:\\Users\\dbjac\\Desktop\\CNN\\CNNimagegen.py:19: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df[\"Return\"].iloc[0] = 0\n",
      "c:\\Users\\dbjac\\Desktop\\CNN\\CNNimagegen.py:22: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df[\"Target\"].iloc[-1] = 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"Stocks\\MSFT\\cleaned_data\\MSFT_2019-01-03_34200000_57600000_orderbook_10.csv\")\n",
    "df = data_frame_organise(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_cnn_2d_model_triple_channel(input_shape):\n",
    "    # Input layer\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Separate channels for Bid, Ask, and Mid\n",
    "    green_channel = layers.Lambda(lambda x: x[:, :, :, 1:2])(inputs)  # Mid-price (Green, channel 1)\n",
    "    red_channel = layers.Lambda(lambda x: x[:, :, :, 0:1])(inputs)    # Ask (Red, channel 0)\n",
    "    blue_channel = layers.Lambda(lambda x: x[:, :, :, 2:3])(inputs)   # Bid (Blue, channel 2)\n",
    "\n",
    "    # Convolution layers for each channel\n",
    "    # Green (Mid) channel - 5x5 kernel\n",
    "    green = layers.Conv2D(32, (5, 5), activation='relu', padding='same')(green_channel)\n",
    "    green = layers.MaxPooling2D((2, 1))(green)\n",
    "    green = layers.Conv2D(64, (5, 5), activation='relu', padding='same')(green)\n",
    "    green = layers.MaxPooling2D((2, 1))(green)\n",
    "\n",
    "    # Red (Ask) channel - 3x3 kernel\n",
    "    red = layers.Conv2D(32, (5, 5), activation='relu', padding='same')(red_channel)\n",
    "    red = layers.MaxPooling2D((2, 1))(red)\n",
    "    red = layers.Conv2D(64, (5, 5), activation='relu', padding='same')(red)\n",
    "    red = layers.MaxPooling2D((2, 1))(red)\n",
    "\n",
    "    # Blue (Bid) channel - 3x3 kernel\n",
    "    blue = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(blue_channel)\n",
    "    blue = layers.MaxPooling2D((2, 1))(blue)\n",
    "    blue = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(blue)\n",
    "    blue = layers.MaxPooling2D((2, 1))(blue)\n",
    "\n",
    "    # Concatenate outputs from all channels\n",
    "    concatenated = layers.Concatenate()([green, red, blue])\n",
    "\n",
    "    # Fully connected layers\n",
    "    flat = layers.Flatten()(concatenated)\n",
    "    dense = layers.Dense(128, activation='relu')(flat)\n",
    "    outputs = layers.Dense(3, activation='softmax')(dense)  # 3 output classes (example)\n",
    "\n",
    "    # Create model\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def create_cnn_2d_model(input_shape):\\n    model = keras.Sequential([\\n        keras.Input(shape=input_shape),\\n        layers.Conv2D(32, (1, 3), activation='relu', padding='same'),\\n        layers.MaxPooling2D((1, 2)),\\n        layers.Conv2D(64, (1, 3), activation='relu', padding='same'),\\n        layers.MaxPooling2D((1, 2)),\\n        layers.Flatten(),\\n        layers.Dense(128, activation='relu'),\\n        layers.Dense(3, activation='softmax')\\n    ])\\n    return model\\n\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def create_cnn_2d_model(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, (1, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((1, 2)),\n",
    "        layers.Conv2D(64, (1, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((1, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_2d_model_vertical(input_shape):\n",
    "    num_features = input_shape[0] \n",
    "    model = keras.Sequential([\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, (num_features, 1), activation='relu', padding='valid'),  \n",
    "        layers.MaxPooling2D((1, 2)),\n",
    "        layers.Conv2D(64, (num_features, 1), activation='relu', padding='valid'), \n",
    "        layers.MaxPooling2D((1, 2)),\n",
    "        layers.Conv2D(128, (num_features, 1), activation='relu', padding='valid'), \n",
    "        layers.MaxPooling2D((1, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target(df, prediction_horizon=10):\n",
    "    target = np.zeros(len(df) - prediction_horizon)  # Initialize target array\n",
    "\n",
    "    for i in range(len(df) - prediction_horizon):\n",
    "      current_price = df['Mid'].iloc[i]\n",
    "      future_price = df['Mid'].iloc[i + prediction_horizon]\n",
    "\n",
    "      if future_price > current_price:\n",
    "          target[i] = 2  # Up\n",
    "      elif future_price < current_price:\n",
    "          target[i] = 0  # Down\n",
    "      else:\n",
    "          target[i] = 1  # Neutral\n",
    "\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(df, grid_function, lookback, batch_size, epochs, prediction_horizon,num_channels):\n",
    "    num_grids = len(df) - lookback\n",
    "    \n",
    "    grids = [grid_function(lookback + i, lookback, df) for i in range(num_grids)]\n",
    "    grids_np = np.array(grids)\n",
    "    print(grids_np.shape)\n",
    "\n",
    "    print(\"Grids initialized\")\n",
    "    grids_reshaped = grids_np.reshape(-1, grids_np.shape[1], grids_np.shape[2], num_channels)\n",
    "    \n",
    "    # Targets\n",
    "    targets = create_target(df,prediction_horizon) \n",
    "\n",
    "    # Train-test split\n",
    "    train_size = int(len(grids_reshaped) * 0.8)  # 80% for training\n",
    "    X_train, X_test = grids_reshaped[:train_size], grids_reshaped[train_size:]\n",
    "    y_train, y_test = targets[:train_size], targets[train_size:]\n",
    "\n",
    "    # Input shape\n",
    "    input_shape = X_train.shape[1:]\n",
    "    if num_channels == 1:\n",
    "        model = create_cnn_2d_model_vertical(input_shape)  # Your CNN model creation function\n",
    "    elif num_channels == 3:\n",
    "        model = create_cnn_2d_model_triple_channel(input_shape)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(\"Training Model\")\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test, y_test),\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "    print(f'Loss: {loss:.4f}')\n",
    "\n",
    "    return history,model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_evaluate(df, grid_function, lookback, batch_size, epochs, num_channels):\n",
    "    num_grids = len(df) - lookback\n",
    "    #DEBUG\n",
    "    og_grid = grid_function(lookback,lookback,df)\n",
    "    # Targets\n",
    "    sample_target = df[\"Target\"].iloc[lookback] + 1\n",
    "    print(sample_target)\n",
    "\n",
    "    grids = [grid_function(lookback + i, lookback, df) for i in range(num_grids)]\n",
    "    grids_np = np.array(grids)\n",
    "    print(grids_np.shape)\n",
    "\n",
    "    print(\"Grids initialized\")\n",
    "    grids_reshaped = grids_np.reshape(-1, grids_np.shape[1], grids_np.shape[2], num_channels)\n",
    "    print(grids_reshaped.shape)\n",
    "    re_grid = grids_reshaped[0]\n",
    "    \n",
    "    return og_grid , re_grid\n",
    "    \n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(197990, 50, 10, 3)\n",
      "Grids initialized\n",
      "Training Model\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 906. MiB for an array with shape (158392, 50, 10, 3) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      4\u001b[0m prediction_horizon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m----> 6\u001b[0m history_v5,model_5 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_image_updown_vol_mid_triple_l10\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookback\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprediction_horizon\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel 5 complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#history_v6 = train_and_evaluate(df, create_image_updown_vol_mid_triple_l5, lookback,batch_size,epochs,3)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#print(\"Model 6 complete\")\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#history_v7,model_v7 = train_and_evaluate(df, create_image_updown_vol_mid_triple_l3, lookback,batch_size,epochs,3)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#print(\"Model 7 complete\")\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[42], line 31\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(df, grid_function, lookback, batch_size, epochs, prediction_horizon, num_channels)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     37\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     40\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optree\\ops.py:747\u001b[0m, in \u001b[0;36mtree_map\u001b[1;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[0;32m    745\u001b[0m leaves, treespec \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39mflatten(tree, is_leaf, none_is_leaf, namespace)\n\u001b[0;32m    746\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treespec\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[1;32m--> 747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreespec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 906. MiB for an array with shape (158392, 50, 10, 3) and data type float32"
     ]
    }
   ],
   "source": [
    "\n",
    "lookback = 10\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "prediction_horizon = 10\n",
    "\n",
    "history_v5,model_5 = train_and_evaluate(df, create_image_updown_vol_mid_triple_l10, lookback,batch_size,epochs,prediction_horizon,3)\n",
    "print(\"Model 5 complete\")\n",
    "#history_v6 = train_and_evaluate(df, create_image_updown_vol_mid_triple_l5, lookback,batch_size,epochs,3)\n",
    "#print(\"Model 6 complete\")\n",
    "#history_v7,model_v7 = train_and_evaluate(df, create_image_updown_vol_mid_triple_l3, lookback,batch_size,epochs,3)\n",
    "#print(\"Model 7 complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasVariable shape=(5, 5, 1, 32), dtype=float32, path=conv2d_18/kernel>,\n",
       " <KerasVariable shape=(32,), dtype=float32, path=conv2d_18/bias>,\n",
       " <KerasVariable shape=(5, 5, 1, 32), dtype=float32, path=conv2d_20/kernel>,\n",
       " <KerasVariable shape=(32,), dtype=float32, path=conv2d_20/bias>,\n",
       " <KerasVariable shape=(3, 3, 1, 32), dtype=float32, path=conv2d_22/kernel>,\n",
       " <KerasVariable shape=(32,), dtype=float32, path=conv2d_22/bias>,\n",
       " <KerasVariable shape=(5, 5, 32, 64), dtype=float32, path=conv2d_19/kernel>,\n",
       " <KerasVariable shape=(64,), dtype=float32, path=conv2d_19/bias>,\n",
       " <KerasVariable shape=(5, 5, 32, 64), dtype=float32, path=conv2d_21/kernel>,\n",
       " <KerasVariable shape=(64,), dtype=float32, path=conv2d_21/bias>,\n",
       " <KerasVariable shape=(3, 3, 32, 64), dtype=float32, path=conv2d_23/kernel>,\n",
       " <KerasVariable shape=(64,), dtype=float32, path=conv2d_23/bias>,\n",
       " <KerasVariable shape=(3456, 128), dtype=float32, path=dense_6/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=dense_6/bias>,\n",
       " <KerasVariable shape=(128, 3), dtype=float32, path=dense_7/kernel>,\n",
       " <KerasVariable shape=(3,), dtype=float32, path=dense_7/bias>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make predictions\n",
    "predictions = model.predict(X_test_images)\n",
    "predicted_labels = np.argmax(predictions, axis=1) #Axis 1 is for each sample\n",
    "accuracy = accuracy_score(y_test_images.astype(int), predicted_labels)\n",
    "print(f\"Test Accuracy (using predictions): {accuracy}\")\n",
    "\n",
    "#Print classification report\n",
    "print(classification_report(y_test_images.astype(int),predicted_labels))\n",
    "\n",
    "#Print confusion matrix\n",
    "print(confusion_matrix(y_test_images.astype(int),predicted_labels))\n",
    "\n",
    "# Compare with model.evaluate (should be the same)\n",
    "loss, accuracy_eval = model.evaluate(X_test_images, y_test_images, verbose=0)\n",
    "print(f\"Test Accuracy (using model.evaluate): {accuracy_eval}\")\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "ax[0].plot(history.history[\"accuracy\"],label=\"Train_Accuracy\")\n",
    "ax[0].plot(history.history[\"accuracy\"],label=\"Val_Accuracy\")\n",
    "ax[1].plot(history.history[\"loss\"],label=\"Train_Loss\")\n",
    "ax[1].plot(history.history[\"val_loss\"],label=\"Val_Loss\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error\n",
    "lookback = 10\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "\n",
    "\n",
    "history_v1 = train_and_evaluate(df, create_image_updown_vol, lookback,batch_size,epochs,1)\n",
    "print(\"Model 1 complete\")\n",
    "history_v2 = train_and_evaluate(df, create_image_side_vol, lookback,batch_size,epochs,1)\n",
    "print(\"Model 2 complete\")\n",
    "history_v3 = train_and_evaluate(df, create_image_updown_price, lookback,batch_size,epochs,1)\n",
    "print(\"Model 3 complete\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
